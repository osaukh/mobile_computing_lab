{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2021-03-24 WS3_1 TensorFlow Detecting Handwritten Digits\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/osaukh/mobile_computing_lab/blob/master/2021-03-24__WS3_1__TensorFlow__Detecting_Handwritten_Digits.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/osaukh/mobile_computing_lab/blob/master/2021-03-24__WS3_1__TensorFlow__Detecting_Handwritten_Digits.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a handwritten character image (MNIST) classifier that can run on any Android device. The app stores a model  set of images (0-9) that we can cycle through and classify in order. It uses a pre-trained model to perform inference on the device. This idea can be applied to any images, both by using the camera and by pulling from the Web. We're using preloaded images so we can run the app in a simulator (no need for the device since it doesn't require a camera).\n",
    "\n",
    "* <a href=\"https://www.youtube.com/watch?v=gahi0Hjgokw\"><b>Video Demo</b></a>\n",
    "* <a href=\"https://github.com/llSourcell/A_Guide_to_Running_Tensorflow_Models_on_Android/tree/master/mnistandroid\"><b>App Source Code</b></a>\n",
    "\n",
    "<img src=\"https://github.com/llSourcell/A_Guide_to_Running_Tensorflow_Models_on_Android/raw/master/images/demo.png\" width=\"800\">\n",
    "\n",
    "\n",
    "## 1. Building / Training / Testing a Model\n",
    "\n",
    "### Data \n",
    "\n",
    "MNIST is a simple computer vision dataset (70'000 labeled examples). It consists of 28x28 pixel images of handwritten digits. Every MNIST data point, every image, can be thought of as an array of numbers describing how dark each pixel is. For example, we might think of 1 as something like:\n",
    "\n",
    "<img src=\"https://tensorflow.rstudio.com/tensorflow/articles/images/MNIST-Matrix.png\" width=\"600\">\n",
    "\n",
    "Since each image has 28 by 28 pixels, we get a 28x28 array. We can flatten each array into a 28∗28=784\n",
    " dimensional vector. Each component of the vector is a value between zero and one describing the intensity of the pixel. Thus, we generally think of MNIST as being a collection of 784-dimensional vectors.\n",
    " \n",
    "The __y_train__ data is the associated labels for all the __x_train__ examples. Rather than storing the label as an integer, it is stored as a 1x10 binary array with the one representing the digit. This is also known as one-hot encoding. In the example below, the array represents a 7:\n",
    "\n",
    "<img src=\"https://d3ansictanv2wj.cloudfront.net/Img-1-array-b4889b9860c9e009bbd58e827a114129.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def load_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    y_train = tensorflow.keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = tensorflow.keras.utils.to_categorical(y_test, 10)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ahinea/work/pyenv/lib/python3.7/site-packages/ipykernel_launcher.py:13: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQMAAAD7CAYAAABuZ/ELAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAANkElEQVR4nO3df6xf9V3H8ecbG3rtSgVK+VVWLhsj1iItyU3KT8nSsqGDTAIYBRMbmEjq1kARUx1hVfmxEnUxrrHQCJuAQwUqg0QjP9bQSSTeCi526lCkpbAuazvWltErLR//OO/C2bXfc8/33u8tve3zkdz0fr/v8/6cz/d7vvf1Pd9zzu2NUgqSdMQHPQFJBwfDQBJgGEhKhoEkwDCQlAwDSYBhICn1NAwi4oSIeC4idkbEH/Vy7C7mUCLi9A61tRHxmZbjvBoRC0c5h9a9OafdEfHcaNYlNYmIhRGxKyLeHek1OWIYdPlDcT2wFZhWSrm5ZY/gs6WUn9t3IyKOjYg1EfFWRGyMiKvbDhQRkyPivojYERFbImJpF70RESsiYlt+rYiI6KL/plznjpzD5JZ9J0XE1yPijQzz/rbrzP55EbE+In6U/87rorc/Ir6Rvf/RzRvARNhOpZSnSylTgU0jjdvrjwmnAt8uo7isMSImtbnvMLES+F/gBOAa4M8iYk7L3uXAx6i2xceB346IS1r2Xg/8IjAXOAu4DPiNNo0R8UlgGbAg1/0R4Pdarvdd4O+BK1ouX1/vkcDjwIPAMcBXgcfz/ja+BrwITAc+DzwSETNa9k647dSolNL4BbwKLMzvFwHfBP4Q+AHwP8DPZ+0rwDtUT84uYCFV2CwD/hvYBvw1cGwu3w8U4Dqq1Houx/9H4Eu5/O3A5FzfJuB7wCrgJ2vzuwX4LvAGcG2OeXqHx7IW+Ex+/1Hg2VzPVuAh4Ohhj/t3gG/nY70f6KvVLwVeAt4EngfO2t9z1uL5fW9OeftD+RyeUbvvAeCLLcd7A/hE7fYfAA+37H0euL52+zrgn1r2/iVwZ+32AmBLm95az6Tcfv1d9HwCeB2I2n2bgEta9J4BDAFH1e5bB9zQondCbac2r8nR7BnMB/4TOA64G/jziIhSyiKqH6i7SylTSylPA5+jSrCLgJOpfqhWDhvvImA28Mna+K9Qpe0dwBdzo80DTgdmArcBZJL+FnAxVcp28xk/gLtyXrOBD1Oldd01Oa+P5hxuzfWeDdxHlcbTgXuAr+9vtzgiLoiIN7uY1xnAnlLKd2r3/Ssw4jtORBwDnJTLd9Wb5vS494SImN6yf7TmAN8q+YpP36LdvOcAr5RSdtbua/uYJ+p26mg0YbCxlLK6lLKXapfsJKof3P25Afh8KWVzKWWI6oftymG7/8tLKW+VUt7O22+UUv60lLIH2E21S3RTKWV7brQ7gV/OZX8JuL+U8m+llLf4/z/MHZVS/quU8lQpZaiU8n3gj6mCqe7LpZTXSinbqYLpV/L+64F7SikvlFL2llK+SvUOc85+1vPNUsrRbecFTAV2DLvvh8BRLXv3Ld9t777+4b1TWx432F8vXax7tIavd9+62z5fY+mdiNupo9F8Jt+y75tSyo9y/VM7LHsqsCYi3q3dt5cfD4/XhvXUb88ApgDra48zgJ/I708G1teW39hi/tUgEScAfwJcSLURjqDac+k0l425Pqge169FxOdq9SNr9bHYBUwbdt80YOd+lt1f777ld3fZu791TwN2DXvX7aaXLtY9WmN9vj6o3n3LH+jt1NF4X2fwGtUxhaNrX32llNdrywx/APXbW4G3gTm1/p8q1dFRqI4VfLi2/Kwu5nZnrutnSynTgF+lCpq64WO/UXtcdwx7XFNKKV/rYv2dfAeYFBEfq903F9gwUmMp5QdUz8ncbnvThh73fq+Usq1l/2htAM4a9q54Fu3mvQH4SETU35HbPuaJup06Gu8wWAXcERGnAkTEjIj4dNvmUsq7wGrgSxFxfI4xM49cQ3VAclFE/ExETAG+0MXcjqJK2B9GxEyqA5HD/WZEnBIRx1Idaf6rvH81cENEzM/TPB+KiE8Ne1GNSn7ceQz4/Rz3fODTVAen2vgL4NaIOCYifhr4daqDu217l+ZzfDJwc5e91+W2OJrq+ErbXiKij+pgMcDkvN3GWqq9zSV5uu6zef+zIzXm5/2XgC9ERF9EXE4VJI+26J2o26mzFkcuX2XY2YRh9feO3ueEbq/VjgCWUh1w3El1VuHOrPVn76Ta8vsbv4/qXfwVqs9o/w4sqdWXUX106fZswhyqjxi7qF4QNwObhz3ufWcT3qQ6PjKlVr8E+OesfRf4G/Ko9LDn7EKqXbhOz+97c6rddyzwt8BbVEfGr67VRhpvMtXBzR1UZ1+W1mqz8vHO6tAbVAeFt+fX3fz4UfpdwIUN616a69xBdfZlcq22AbimobcM/6rVVgGrGnrPzm35NvAvwNm12u8Cf9fQ25/b4O18nS6s1a4BNjT0TpjtRIuzCVHG9jFDYxQR/wCcCwyWUj7+Qc9Hh5aIWEC1pzMZ+IVSyjc6LmsYSAJ/UUlSMgwkAYaBpDQuvwh03HHHlf7+/vEYWlJav3791lJK21+qGtG4hEF/fz+Dg4PjMbSkFBGtr7htw48JkgDDQFIyDCQBhoGkZBhIAgwDSckwkAQYBpKSYSAJMAwkJcNAEmAYSEqGgSTAMJCUDANJgGEgKRkGkgDDQFIyDCQBhoGkZBhIAgwDSckwkAQYBpKSYSAJMAwkJcNAEmAYSEqGgSTAMJCUDANJgGEgKRkGkgDDQFIyDCQBhoGkNOmDnoDet27dusb6VVdd1bH20ksvNfaeeOKJo5qTDh/uGUgCDANJyTCQBBgGkpJhIAkwDCQlw0AS4HUGB5U1a9Y01iPiAM1kYtm9e3fHWl9f3wGcycTmnoEkwDCQlAwDSYBhICkZBpIAw0BS8tTiAbR58+bG+gMPPDDqsffu3Tvq3oPdww8/3Fh/8cUXO9ZWrFjR6+kcstwzkAQYBpKSYSAJMAwkJcNAEmAYSEqGgSTA6wwOqNdff72xvm3btsb6+eef37E2c+bMUc3pYLBz587G+uLFixvrd911Vy+nc9hyz0ASYBhISoaBJMAwkJQMA0mAYSApGQaSAK8zOKDG+rv1N954Y49mcnC55557xtS/YMGCHs3k8OaegSTAMJCUDANJgGEgKRkGkgDDQFIyDCQBEKWUng86MDBQBgcHez7uwa7p/+8HOO+88xrrTX9aHGA8ttWBsHXr1sb6jBkzGusXXXRRY33t2rXdTumQEBHrSykDvRrPPQNJgGEgKRkGkgDDQFIyDCQBhoGkZBhIAvz/DHpq06ZNjfWhoaHGekT0cjoH1DvvvNOxdttttzX2jvS477vvvlHNSd1xz0ASYBhISoaBJMAwkJQMA0mAYSApeWpRPfHCCy90rK1ataqxd/78+Y31U045ZVRzUnfcM5AEGAaSkmEgCTAMJCXDQBJgGEhKhoEkwOsMemrq1KmN9UmTmp/uPXv2NNaffvrpjrWFCxc29o63pmsJRnpeVq9e3Vg/8sgjRzUndcc9A0mAYSApGQaSAMNAUjIMJAGGgaRkGEgCvM6gpxYsWNBYX758eWP91ltvbaxfeeWVHWuPPPJIY+9Yr0N48sknG+sPPfRQx9oFF1zQ2HvmmWeOak7qLfcMJAGGgaRkGEgCDANJyTCQBBgGkpJhIAnwOoMDavHixY31wcHBxvqaNWs61i677LLG3mXLljXWR7pG4vnnn2+sN7n88stH3TtWL7/8cmP9mWeeaayfc845jfV58+Z1PaeDlXsGkgDDQFIyDCQBhoGkZBhIAgwDSckwkARAlFJ6PujAwEAZ6Zy5unfttdd2rD344IONvSP9TYaxanodzZ49u7F30aJFPZ7N+x599NHG+pQpUxrrTzzxRGN9pL8JMZ4iYn0pZaBX47lnIAkwDCQlw0ASYBhISoaBJMAwkJQ8tXiIePbZZxvrjz/+eGP93nvvbawPDQ011pteRxHR2DueRvoV45UrVzbWzz333F5Op6c8tShpXBgGkgDDQFIyDCQBhoGkZBhIAgwDScnrDATA8ccf31ifPn16Y73p16v7+voae6+++urGetOfewe4+OKLO9ZOO+20xt6R5nYw8zoDSePCMJAEGAaSkmEgCTAMJCXDQBJgGEhK/kn2w8S6desa6zt27Gisj3SdwS233NL1nNpasmTJuI2t97lnIAkwDCQlw0ASYBhISoaBJMAwkJQMA0mA1xkcNrZv395YnzSp+aVw++2393I6Ogi5ZyAJMAwkJcNAEmAYSEqGgSTAMJCUPLV4mHjsscca67NmzWqsX3HFFb2cjg5C7hlIAgwDSckwkAQYBpKSYSAJMAwkJcNAEuB1BoeMkf6r86eeeqqxPnfu3F5ORxOQewaSAMNAUjIMJAGGgaRkGEgCDANJyTCQBHidwSHj/vvvb6xv2bKlsX7TTTf1cjqagNwzkAQYBpKSYSAJMAwkJcNAEmAYSEqGgSTA6wwOGUNDQ2Pqv/TSS3s0E01U7hlIAgwDSckwkAQYBpKSYSAJMAwkJcNAEgBRSun5oAMDA2VwcLDn40p6X0SsL6UM9Go89wwkAYaBpGQYSAIMA0nJMJAEGAaSkmEgCTAMJCXDQBJgGEhKhoEkwDCQlAwDSYBhICkZBpIAw0BSMgwkAYaBpGQYSAIMA0nJMJAEGAaS0rj8V+kR8X1gY88HllR3aillRq8GG5cwkDTx+DFBEmAYSEqGgSTAMJCUDANJgGEgKRkGkgDDQFIyDCQB8H96NaBy0SFllwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def display_digit(digit, title = \"\"):\n",
    "    \"\"\"\n",
    "    graphically displays a 784x1 vector, representing a digit\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    fig = plt.imshow(digit.flatten().reshape(28,28))\n",
    "    fig.set_cmap('gray_r')\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "    if title != \"\":\n",
    "        plt.title(\"Inferred label: \" + str(title))\n",
    "    plt.show()\n",
    "        \n",
    "digit = 142\n",
    "display_digit(x_train[digit], title = y_train[digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model\n",
    "\n",
    "Read: <a href=\"https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59\">Visualizing parts of Convolutional Neural Networks using Keras and Cats</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=64, kernel_size=3, strides=1, \\\n",
    "            padding='same', activation='relu', \\\n",
    "            input_shape=[28, 28, 1]))\n",
    "    # 28*28*64\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "    \n",
    "    # 14*14*64\n",
    "    model.add(Conv2D(filters=128, kernel_size=3, strides=1, \\\n",
    "            padding='same', activation='relu'))\n",
    "\n",
    "    # 14*14*128\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "    \n",
    "    # 7*7*128\n",
    "    model.add(Conv2D(filters=256, kernel_size=3, strides=1, \\\n",
    "            padding='same', activation='relu'))\n",
    "    \n",
    "    # 7*7*256\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "    \n",
    "    # 4*4*256\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 4,575,242\n",
      "Trainable params: 4,575,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training / Testing / Validation\n",
    "\n",
    "When solving with a CPU an Optimization Problem, you Iteratively apply an Algorithm over some Input Data. In each of these iterations you usually update a Metric of your problem doing some Calculations on the Data. Now when the size of your data is large it might need a considerable amount of time to complete every iteration, and may consume a lot of resources. So sometimes you choose to apply these iterative calculations on a Portion of the Data to save time and computational resources. This portion is the batch_size and the process is called (in the Neural Network Lingo) batch data processing.\n",
    "\n",
    "In the neural network terminology:\n",
    "* one __epoch__ = one forward pass and one backward pass of all the training examples\n",
    "* __batch size__ = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.\n",
    "* number of __iterations__ = number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).\n",
    "\n",
    "Advantages:\n",
    "* It requires less memory. Since you train network using less number of samples the overall training procedure requires less memory. It's especially important in case if you are not able to fit dataset in memory.\n",
    "* Typically networks trains faster with mini-batches. That's because we update weights after each propagation. In our example we've propagated 11 batches (10 of them had 100 samples and 1 had 50 samples) and after each of them we've updated network's parameters. If we used all samples during propagation we would make only 1 update for the network's parameter.\n",
    "\n",
    "Disadvantages:\n",
    "* The smaller the batch the less accurate estimate of the gradient. In the figure below you can see that mini-batch (green color) gradient's direction fluctuates compare to the full batch (blue color).\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/lU3sx.png\" width=\"700\">\n",
    "\n",
    "Training code below takes around 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1352 - accuracy: 0.9582\n",
      "Epoch 00001: saving model to ws3_hwd_data/cp.ckpt\n",
      "60000/60000 [==============================] - 71s 1ms/sample - loss: 0.1350 - accuracy: 0.9582 - val_loss: 0.0354 - val_accuracy: 0.9878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x161690850>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "model.compile(loss=tensorflow.keras.losses.categorical_crossentropy, \\\n",
    "    optimizer=tensorflow.keras.optimizers.Adam(), \\\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "checkpoint_path = \"ws3_hwd_data/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tensorflow.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# Train the model with the new callback\n",
    "model.fit(x_train, y_train, \\\n",
    "        batch_size=BATCH_SIZE, \\\n",
    "        epochs=EPOCHS, \\\n",
    "        verbose=1, \\\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=[cp_callback])  # Pass callback to training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. TensorFlow Lite\n",
    "\n",
    "#### TensorFlow Lite Converter\n",
    "\n",
    "We use the TensorFlow Lite converter to convert the model into an optimized one for mobile devices. TensorFlow Lite uses the optimized [FlatBuffers](https://google.github.io/flatbuffers/) format to represent graphs. FlatBuffers play an essential role in efficiently serializing model data and providing quick access to that data while maintaining a small binary size. This is particularly useful for model files that are heavily populated with numerical weight data that can, by virtue of their size, create a lot of latency in read operations. Using FlatBuffer protocols as the basis for this transformation, TensorFlow Lite can bypass a lot of the traditionally expensive file parsing and un-parsing that contributes to slower execution.\n",
    "\n",
    "Therefore, a TensorFlow model needs to be converted into a FlatBuffer file before deploying it to Android. The TensorFlow Lite converter generates a TensorFlow Lite FlatBuffer file (.tflite) from a TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18303964"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = tensorflow.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "open(\"ws3_hwd_data/converted_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow Lite Interpreter\n",
    "\n",
    "The Interpreter Core is responsible for executing Lite models in client applications using a reduced set of TensorFlow’s operators. By limiting the default operators, libraries, and tools required to run the Lite models, the Interpreter Core has been trimmed to a lean ~100kb alone or ~300kb with all supported kernels.\n",
    "\n",
    "#### Hardware Acceleration\n",
    "\n",
    "TensorFlow Lite optimizations reach all the way down to the hardware. Working within the tight constraints of mobile and embedded devices means that processors must be utilized at a hyper-efficient standard. The Android NDK contains a Neural Network API (NNAPI) that provides access to hardware-accelerated inference operations on Android devices. The NNAPI interfaces with TensorFlow Lite to seek out paths for model operations to leverage advantageous hardware where available. With the expectation of machine learning hardware becoming more available on Edge devices, the advantages of the NNAPI framework will become more apparent.\n",
    "\n",
    "In 2019, the TensorFlow team released support for a GPU backend that will allow for a subset of models and operations to selectively utilize GPUs on mobile devices. The update will benefit models with excess parallelizable work and those that suffer from quantization precision-loss, giving them increases in speed and efficiency (up to 7x for some neural nets). The TensorFlow team promises to increase the set of models and operators currently supported to provide more comprehensive coverage in the future.\n",
    "\n",
    "#### Quantization\n",
    "\n",
    "Quantization in TensorFlow Lite refers to the process of reducing operation precision from 32-bit floating point numbers to 8-bit values. Post-training quantization is encouraged in TensorFlow Lite and is provided as an attribute of the TensorFlow Lite Converter during the conversion step. Benchmarking has shown that compressed model inference latency can be reduced up to 3x while maintaining a negligible drop in inference accuracy.\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/lite/images/performance/model_size_vs_accuracy.png\" width=\"600\"/>\n",
    "<img src=\"https://www.tensorflow.org/lite/images/performance/accuracy_vs_latency.png\" width=\"600\"/>\n",
    "\n",
    "Aggregated, these core optimizations provide us a reliable framework within which we can continue to probe the frontiers of on-device machine learning.\n",
    "\n",
    "***\n",
    "## 3. Android App\n",
    "\n",
    "NDK (Native Development Kit) is a tool that allows you to program in C/C++ for Android devices. It integrates with the SDK and is used for performance-critical code.\n",
    "\n",
    "* <a href=\"https://developer.android.com/ndk/downloads/\">Download and install Android NDK</a> (you should actually get an automatic invitation to install NDK from Android Studio once you try to build an app which requires its support).\n",
    "\n",
    "![alt text](https://jalammar.github.io/images/android-tensorflow-app-structure_2.png)\n",
    "\n",
    "* The HandWrittenDigits Android app is provided here: <a href=\"https://github.com/osaukh/mobile_computing_lab/tree/master/code/HandWrittenDigits\">source code</a>. It requires that you use TensorFlow verion > 1.9.1. Examine the code and run it on your hardware. You can also experiment with the model above and use your own model (copy your .tflite file to HandWrittenDigits/app/src/main/assets/).\n",
    "\n",
    "\n",
    "***\n",
    "## Credits, Related Examples and Useful Links\n",
    "\n",
    "* [TensorFlow Lite guide](https://www.tensorflow.org/lite/guide)\n",
    "* <a href=\"https://www.oreilly.com/learning/not-another-mnist-tutorial-with-tensorflow\">Not another MNIST tutorial with TensorFlow</a>\n",
    "* <a href=\"https://heartbeat.fritz.ai/intro-to-machine-learning-on-android-how-to-convert-a-custom-model-to-tensorflow-lite-e07d2d9d50e3\">Intro to Machine Learning on Android — How to convert a custom model to TensorFlow Lite</a>\n",
    "* <a href=\"https://heartbeat.fritz.ai/introduction-to-machine-learning-on-android-part-2-building-an-app-to-recognize-handwritten-d58ebc01950\">Intro to Machine Learning on Android (Part 2): Building an app to recognize handwritten digits with TensorFlow Lite</a>\n",
    "* Medium [How TensorFlow Lite Optimizes Neural Networks for Mobile Machine Learning](https://heartbeat.fritz.ai/how-tensorflow-lite-optimizes-neural-networks-for-mobile-machine-learning-e6ffa7f8ee12), 2019\n",
    "* YouTube video by Siraj Raval: <a href=\"https://www.youtube.com/watch?v=kFWKdLOxykE&feature=youtu.be\">A Guide to Running Tensorflow Models on Android</a>\n",
    "* <a href=\"https://github.com/llSourcell/A_Guide_to_Running_Tensorflow_Models_on_Android\">Original app source code for older versions of TensorFlow (<= 1.5.0)</a> adjusted by Siraj Raval\n",
    "* Original <a href=\"https://github.com/miyosuda/TensorFlowAndroidMNIST\">app source code</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
