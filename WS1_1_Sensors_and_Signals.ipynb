{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 1: Sensors and Signals\n",
    "\n",
    "## 1. Install Android Studio on Your Laptop\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "__Follow instructions for your operating system__: \n",
    "<ul><li>\n",
    "<a href=\"https://developer.android.com/studio/install.html\">Install Android Studio</a>\n",
    "</li></ul>\n",
    "</div>\n",
    "\n",
    "\n",
    "## 2. Android Hello World App\n",
    "\n",
    "Goal: Make sure you have a working system, create your first Android app and run it on an emulator and on your phone.\n",
    "\n",
    "<img src=\"http://mateoj.com/public/images/movies-app/screenshot_step1.png\" width=\"250\">\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "__Follow instructions__:\n",
    "<ul><li>\n",
    "Read: <a href=\"https://google-developer-training.gitbooks.io/android-developer-fundamentals-course-concepts/content/en/Unit%201/11_c_create_your_first_android_app.html\">1.1: Create Your First Android App</a>\n",
    "</li></ul>\n",
    "</div>\n",
    "\n",
    "## 3. Sensors and Signals\n",
    "\n",
    "Goal: Learn about the Android sensor framework, which is used to find the available sensors on a device and retrieve data from those sensors.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"https://google-developer-training.gitbooks.io/android-developer-advanced-course-practicals/images/3-1-p-working-with-sensor-data/pv_sensorsurvey-screenshot.png\" width=\"250\"></td>\n",
    "<td><img src=\"https://google-developer-training.gitbooks.io/android-developer-advanced-course-practicals/images/3-1-p-working-with-sensor-data/pv_sensorlisteners-screenshot.png\" width=\"250\"></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "__Follow instructions__:\n",
    "<ul>\n",
    "<li> Read: <a href=\"https://developer.android.com/guide/topics/sensors/sensors_overview.html\">Sensors overview</a> </li>\n",
    "<li> Follow the steps: <a href=\"https://google-developer-training.gitbooks.io/android-developer-advanced-course-practicals/unit-1-expand-the-user-experience/lesson-3-sensors/3-1-p-working-with-sensor-data/3-1-p-working-with-sensor-data.html\">3.1: Working with sensor data</a> </li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "\n",
    "## 4. Activity Monitoring\n",
    "\n",
    "<img src=\"img/ws1/project_option_1_person_walking.png\" width=\"600\">\n",
    "\n",
    "#### Step 1:  Raw signal\n",
    "* Key sensors for our app (... and many more other applications):\n",
    "    * **Accelerometers** convert acceleration into an electrical signal\n",
    "    * **Gyroscopes** measure angular rate (how quickly an object turns)\n",
    "    * **Compass** sensor detect the magnetic field of the Earth\n",
    "    * Radio transceivers detect surrounding access points and their **signal strength (RSS)**\n",
    "* Energy consumption matters\n",
    "    * For example, on HP iPAQ hw6965: GPS 620 mW, Microphone 225 mW, Accelerometer 2 mW\n",
    "    * **In general, always solve the problem using as little energy as possible**\n",
    "* Most smartphones have 3-axis accelerometers. Accelerometers can provide a lot of information.\n",
    "    * Activity monitoring\n",
    "<img src=\"img/ws1/project_option_1_acceleration_data.png\">\n",
    "Source: Paper <a href=\"http://web.media.mit.edu/~intille/papers-files/BaoIntille04.pdf\">\"Activity Recognition from User-Annotated Acceleration Data\"</a>, Pervasive 2004\n",
    "    * Detect potholes and traffic conditions\n",
    "<img src=\"img/ws1/project_option_1_traffic_monitoring.png\">\n",
    "Source: Paper <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/Nericell-Sensys2008.pdf\">\"Nericell: Rich Monitoring of Road and Traffic Conditions using Mobile Smartphones\"</a>, ACM SenSys 2008\n",
    "    * Many more applications ...\n",
    "\n",
    "\n",
    "\n",
    "#### Step 2: Feature extraction (art and science)\n",
    "\n",
    "* How would you describe a signal statistically?\n",
    "* Example:\n",
    "    * Choose a window size (20 samples, 500 ms)\n",
    "    * Select features (mean, max min, variance, Fourier transforms, autocorrelation, ...)\n",
    "\n",
    "<img src=\"img/ws1/project_option_1_acceleration_data2.png\" width=\"600\">\n",
    "\n",
    "\n",
    "\n",
    "#### Step 3: Classification method (science)\n",
    "\n",
    "* Get some features and plot them. There going to be an overlap depending on the window's size.\n",
    "\n",
    "<img src=\"img/ws1/project_option_1_classification.png\">\n",
    "Source: Paper <a href=\"http://web.media.mit.edu/~intille/papers-files/BaoIntille04.pdf\">\"Activity Recognition from User-Annotated Acceleration Data\"</a>, Pervasive 2004\n",
    "\n",
    "* **k-Nearest Neighbors algorithm (K-NN)**\n",
    "\n",
    "K-NN is a top-10 machine learning tool! The output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/220px-KnnClassification.svg.png\" width=\"300\">\n",
    "\n",
    "The test sample (green circle) should be classified either to the first class of blue squares or to the second class of red triangles. If k = 3 (solid line circle) it is assigned to the second class because there are 2 triangles and only 1 square inside the inner circle. If k = 5 (dashed line circle) it is assigned to the first class (3 squares vs. 2 triangles inside the outer circle).\n",
    "\n",
    "Source: <a href=\"http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\">Wikipedia</a>\n",
    "\n",
    "Decisions to be made:\n",
    "* What is K?\n",
    "    * Odd number\n",
    "    * $\\sqrt{N}$\n",
    "\n",
    "* What is distance?\n",
    "    * **Euclidean**\n",
    "    The Euclidean distance or Euclidean metric is the \"ordinary\" (i.e.straight-line) distance between two points in Euclidean space.\n",
    "    $$D(w_i, v_i) = \\sqrt{\\sum (w_i-v_i)^2}$$\n",
    "    * **Manhattan** \n",
    "    The Manhattan distance, also known as rectilinear distance, city block distance, taxicab metric is defined as the sum of the lengths of the projections of the line segment between the points onto the coordinate axes.\n",
    "    $$D(w_i, v_i) = \\sum | w_i-v_i |$$\n",
    "        * Example:\n",
    "    <img src=\"img/ws1/project_option_1_manhatten.png\" width=\"200\">\n",
    "        Source: <a href=\"http://www.isumitjha.com/2017/12/chebyshev-vs-euclidean-vs-manhattan.html\">Euclidean vs Chebyshev vs Manhattan Distance?</a>\n",
    "    * **Chebyshev (Chessboard)**\n",
    "    In Chebyshev distance, all 8 adjacent cells from the given point can be reached by one unit i.e diagonal move is valid. It is also known as chessboard distance, since in the game of chess the minimum number of moves needed by a king to go from one square on a chessboard to another equals the Chebyshev distance between the centers of the squares.\n",
    "    $$D(w_i, v_i) = \\max | w_i-v_i |$$\n",
    "        * Example:\n",
    "    <img src=\"img/ws1/project_option_1_chebyshev.png\" width=\"200\">\n",
    "        Source: <a href=\"http://www.isumitjha.com/2017/12/chebyshev-vs-euclidean-vs-manhattan.html\">Euclidean vs Chebyshev vs Manhattan Distance?</a>\n",
    "    \n",
    "    * **Hamming**\n",
    "    In information theory, the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other.\n",
    "    $$D(w_i, v_i) = \\sum I (w_i-v_i)$$\n",
    "        * Example:\n",
    "        <img src=\"https://www.researchgate.net/profile/Fredrick_Ishengoma/publication/264978395/figure/fig1/AS:295895569584128@1447558409105/Example-of-Hamming-Distance.png\" width=\"300\">\n",
    "        * Used in localization: Paper <a href=\"http://www.inf.ed.ac.uk/teaching/courses/cn/papers/sensloc.pdf\">\"SensLoc: Sensing Everyday Places and Paths using Less Energy\"</a>, ACM Sensys 2010\n",
    "\n",
    "    * **Mahalanobis** \n",
    "    Mahalanobis distance is an Euclidian distance which takes the covariance of data into account. A very good explanation can be found here: <a href=\"http://stats.stackexchange.com/questions/62092/bottom-to-top-explanation-of-the-mahalanobis-distance\">Bottom to top explanation of the Mahalanobis distance?</a>\n",
    "    $$D(w_i, v_i) = \\sqrt{(w-v)^T S^{-1} (w-v)}$$\n",
    "        * Example:<img src=\"img/ws1/project_option_1_mahalanobis.png\">\n",
    "        * Used in localization: Paper <a href=\"https://www.cs.ox.ac.uk/files/6677/MapCraftPublishedIPSN14.pdf\">\"Lightweight Map Matching for Indoor Localisation Using Conditional Random Fields\"</a>, IEEE/ACM IPSN 2014.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "# Your Task: Activity Monitoring (Option 1)\n",
    "\n",
    "<img src=\"img/ws1/project_option_1_activity_monitoring.png\" width=\"600\">\n",
    "\n",
    "## Cookbook\n",
    "\n",
    "#### Step 1: Read accelerometer data and display data on screen\n",
    "\n",
    "* Implement the interface SensorEventListener\n",
    "\n",
    "```Java\n",
    "public class MainActivity extends AppCompatActivity implements SensorEventListener {\n",
    "```\n",
    "\n",
    "* Declare variables for SensorManager and accelerometer \n",
    "\n",
    "```Java\n",
    "    private SensorManager sm;\n",
    "    private Sensor accelerometer;\n",
    "```\n",
    "\n",
    "* Instantiate them in OnCreate method\n",
    "\n",
    "```Java\n",
    "@Override\n",
    "protected void onCreate(Bundle savedInstanceState) {\n",
    "\t\t...\n",
    "    sm = (SensorManager)getSystemService(SENSOR_SERVICE);\n",
    "    accelerometer = sm.getDefaultSensor(Sensor.TYPE_ACCELEROMETER);\n",
    "\t\t...\n",
    "\t\t\n",
    "    sm.registerListener(this, accelerometer, SensorManager.SENSOR_DELAY_NORMAL);\n",
    "}\n",
    "```\n",
    "\n",
    "* Capture sensor values in onSensorChanged method\n",
    "\n",
    "```Java\n",
    "@Override\n",
    "public void onSensorChanged(SensorEvent event) {\n",
    "    double x, y, z;\n",
    "\tx = event.values[0];\n",
    "\ty = event.values[1];\n",
    "\tz = event.values[2];\n",
    "\t\t\n",
    "\t//Store data in memory, file, or in other data structure\n",
    "\taddDataToProcess (x, y, z); \n",
    "}\n",
    "```\n",
    "\n",
    "* Call your function to recognize activity at suitable times\n",
    "\n",
    "```Java\n",
    "public ActivityType recognizeActivity () {\n",
    "\t// ActivityType is an enum {NONE, SIT, WALK, RUN, ...};\n",
    "\t\t\n",
    "\t// Fill in with your algorithm\n",
    "}\n",
    "```\n",
    "  \n",
    "#### Step 2:  Write accelerometer data to a file\n",
    "\n",
    "* File I/O - Reading\n",
    "\n",
    "```Java\n",
    "import java.io.DataInputStream;\n",
    "import java.io.File;\n",
    "import java.io.FileInputStream;\n",
    "import java.io.FileNotFoundException;\n",
    "import java.io.IOException;\n",
    "\n",
    "//Reading from a file\n",
    "DataInputStream fInpStream = null;\n",
    "\n",
    "try {\n",
    "\tfInpStream = new DataInputStream (new FileInputStream(\"<path>\"));\n",
    "\twhile(fInpStream.available() > 0) {\n",
    "\t\tdouble d = fInpStream.readDouble ();\n",
    "\t\t...\n",
    "\t}\n",
    "}\n",
    "catch (FileNotFoundException e) {\n",
    "\te.printStackTrace();\n",
    "\t...\n",
    "}\n",
    "```\n",
    "\n",
    "* File I/O - Writing\n",
    "\n",
    "```Java\n",
    "import java.io.DataOutputStream;\n",
    "import java.io.File;\n",
    "import java.io.FileNotFoundException;\n",
    "import java.io.FileOutputStream;\n",
    "import java.io.IOException;\n",
    "\n",
    "//Writing to a file\n",
    "DataOutputStream fOutStream = null;\n",
    "\n",
    "try {\n",
    "\tfOutStream = new DataOutputStream (new FileOutputStream(\"<path>\"));\n",
    "\tdouble d = 0.02;\n",
    "\tfOutStream.writeDouble(d);\n",
    "\tfOutStream.flush();\n",
    "} catch (FileNotFoundException e) {\n",
    "\te.printStackTrace();\n",
    "\t...\n",
    "} catch (IOException e) {\n",
    "\te.printStackTrace();\n",
    "\t...\n",
    "}\n",
    "```\n",
    "\n",
    "Also see: <a href=\"https://www.journaldev.com/9400/android-external-storage-read-write-save-file\">Android External Storage â€“ Read, Write, Save File</a>\n",
    "\n",
    "You need to make sure that the application has permission to read and write data to the users SD card, so open up the AndroidManifest.xml and add the following permissions:\n",
    "\n",
    "```XML\n",
    "<uses-permission android:name=\"android.permission.WRITE_INTERNAL_STORAGE\"/>\n",
    "<uses-permission android:name=\"android.permission.READ_INTERNAL_STORAGE\"/>\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```XML\n",
    "<uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\"/>\n",
    "<uses-permission android:name=\"android.permission.READ_EXTERNAL_STORAGE\"/>\n",
    "```\n",
    "\n",
    "\n",
    "#### Step 3: Copy file from step 3) to your laptop\n",
    "\n",
    "#### Step 4: Do K-NN analysis offline first\n",
    "You can use any programming language you want! I highly recommend to use Python.\n",
    "* Divide data in two parts for training and testing\n",
    "* Select a window size\n",
    "* Select feature vectors from training set\n",
    "* Perform classification with testing set\n",
    "\n",
    "#### Step 5: Program K-NN in your smartphone and display classification on screen\n",
    "\n",
    "Example:\n",
    "<img src=\"https://aqibsaeed.github.io/img/har_app_screenshot.png\" width=\"250\">\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Please do NOT use Google Play Services Activity Recognition API (see <a href=\"https://code.tutsplus.com/tutorials/how-to-recognize-user-activity-with-activity-recognition--cms-25851\">\"How to Recognize User Activity With Activity Recognition\"</a>), but you can use it to compare your own app performance.\n",
    "</div>\n",
    "\n",
    "\n",
    "***\n",
    "# References\n",
    "\n",
    "* __Starting with Android__\n",
    "    * <a href=\"http://developer.android.com/sdk/index.html\">Android SDK</a>\n",
    "    * <a href=\"http://developer.android.com/training/basics/firstapp/index.html\">Android Tutorial</a>\n",
    "    * __Android Developer Fundamentals Course__\n",
    "        * <a href=\"developer.google.com/training/adf\">Course home page</a>\n",
    "        * <a href=\"https://www.gitbook.com/book/google-developer-training/android-developer-fundamentals-course-concepts/details\">Tutorial</a>\n",
    "    * __Android Developer Advanced Course__\n",
    "        * <a href=\"https://google-developer-training.gitbooks.io/android-developer-advanced-course-practicals/\">Tutorial</a>\n",
    "\n",
    "* __K-NN__\n",
    "    * <a href=\"https://www.cs.rit.edu/~rlaz/PatternRecognition/slides/kNearestNeighbor.pdf\">Nearest Neighbor Rule</a>\n",
    "    * <a href=\"http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/tutorials/MIT6_034F10_tutor03.pdf\">KNN, ID Trees, and Neural Nets</a>\n",
    "    * <a href=\"http://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/\">A Detailed Introduction to K-Nearest Neighbor (KNN) Algorithm</a>\n",
    "\n",
    "\n",
    "***\n",
    "# Credits\n",
    "\n",
    "* Marco Zuniga's <a href=\"http://studiegids.tudelft.nl/a101_displayCourse.do?course_id=40368\">\"Smart Phone Sensing\" Course at TU Delft</a>\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "# Next\n",
    "\n",
    "* <a href=\"WS2_1_Localization.ipynb\">Workshop 2 -- Indoor Localization: Bayesian fitler, particle filter, sensor fusion</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
