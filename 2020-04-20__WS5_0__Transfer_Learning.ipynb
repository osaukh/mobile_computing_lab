{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-04-20 WS5_0 Transfer Learning on Mobile Devices\n",
    "\n",
    "## Activity Monitoring: What should you have by now?\n",
    "\n",
    "If you decided to go for Option 1, you should have a mobile app providing the following functionality:\n",
    "* Gathering sensor data (from accelerometer, gyroscope, etc.)\n",
    "* Extracting features from this data\n",
    "* Using kNN to attribute the feature vector to the right activity. The model parameters are determined offline using your own data set.\n",
    "\n",
    "Your app should be fairly accurate when tested with a smartphone held and fixed in the same position on your body as when gathering training data. If you change the position (e.g., by rotating the phone or fixing it to your leg instead of your arm) your activitiy recognition will most probably fail. In this workshop you will learn how one could address the problem with __transfer learning__ on your phone. We will create a general model and personalize it on the phone.\n",
    "\n",
    "<img src=\"img/sensors/project_option_1_acceleration_data.png\" width=\"600\">\n",
    "\n",
    "\n",
    "## What is Transfer Learning?\n",
    "\n",
    "Transfer learning is inspired by the way human learners take advantage of their existing knowledge and skills: A human who knows how to read literature is more likely to succeed in reading scientific papers than a human who does not know how to read at all. In the context of supervised learning, transfer learning implies the ability to reuse the knowledge of the dependency structure between features and labels learned in one setting to improve the inference of the dependency structure in another setting.\n",
    "\n",
    "More formally, given a source domain with data distribution $Q_S$ and its corresponding task $T_S$ and a target domain $Q_T$ withtask $T_T$, transfer learning aims to support the learning of the target task $T_T$ in $Q_T$ using the knowledge of $Q_S$ and $T_S$, where $Q_S \\neq Q_T$, or $T_S \\neq T_T$. Transfer learning is mainly concerned with the forward transfer desiderata of __continual learning__. However, it doesn’t involve any continuous adaptation after learning the target task. Moreover, the performance on the source task(s) is not taken into account during transfer learning. A quite popular example of transfer learning is finetuning, where models pre-trained on large tasks are used as initialization for tasks with limited training data.\n",
    "\n",
    "__Domain adaptation__ is a sub-field of transfer learning where _the source and target tasks are the same_ but drawn from different input domains. The target domain data is unlabelled and the goal is to adapt a model trained on the source domain to perform well on the unlabelled target domain. In other words, it relaxes the classical machine learning assumption of having training and test data drawn from the same distribution. As mentioned above for transfer learning, domain adaptation is unidirectional and doesn’t involve any accumulation of knowledge.\n",
    "\n",
    "## On-device Model Personalization with TensorFlow Lite\n",
    "\n",
    "Suppose that you want to get the best user experience possible by adjusting the model to users’ needs (specific position or way of carrying a smartphone, etc.). Sending user data to the cloud to train the model requires a lot of care to prevent potential privacy breaches. It's not always practical to send data to a central server for training---issues like power, data caps, and privacy can be problematic. However, training directly on device is a strong approach with many benefits: Privacy-sensitive data stays on the device so it saves bandwidth, and it works without an internet connection.\n",
    "\n",
    "This poses a __challenge__: _Training can require a non-trivial number of data samples which is hard to get on-device. Training a deep network from scratch can take days on the cloud so it's not suitable on device._ Instead of training a whole new model from scratch, we can retrain an already trained model to adapt to a similar problem through transfer learning.\n",
    "\n",
    "Transfer learning involves using a pre-trained model for one “data-rich” task and retraining a part of its layers (typically the last ones) to solve another, “data-poor” task.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*Bi-_rpBbfhVAz6gi1uIRnw.png\" width=\"800\">\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Using transfer learning, you can easily train personalized models on-device even with <b>limited training data and computational resources</b>, all while <b>preserving user privacy</b>.\n",
    "</div>\n",
    "\n",
    "## Example: Train an Image Classifier on Your Android Device\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "This application example is taken from the TensorFlow Lite tutorial:\n",
    "<ul><li>\n",
    "<a href=\"https://github.com/tensorflow/examples/tree/master/lite/examples/model_personalization\">TensorFlow Lite Example On-device Model Personalization</a>\n",
    "</li></ul>\n",
    "</div>\n",
    "\n",
    "The example includes an Android application that learns to classify camera images in real-time. The training is performed on-device by taking sample photos of different target classes.\n",
    "\n",
    "<img src=\"https://3.bp.blogspot.com/-UZq9OA4Kz4s/XehrONzQvYI/AAAAAAAABdA/DDjNRBeuSkkrjtV13bb2tZYL5gQrdxX_ACLcBGAsYHQ/s1600/Screenshot_20190820-163122.png\" width=\"200\">\n",
    "\n",
    "The app uses transfer learning on a quantized MobileNetV2 model pre-trained on ImageNet with the last few layers replaced by a trainable softmax classifier. You can train the last layers to recognize any four new classes. Accuracy depends on how “hard” the classes are to capture. For many objects just tens of samples can be enough to achieve good results. Compare this to ImageNet, which has 1.3 million samples!\n",
    "\n",
    "This example includes a set of easily reusable tools that make it easy to create and use your own personalizable models. The example includes three distinct and isolated parts, each of them responsible for a single step in the transfer learning pipeline.\n",
    "\n",
    "<img src=\"https://4.bp.blogspot.com/-vfNgyvvXghI/Xehr7LQTXNI/AAAAAAAABdM/OM7OY69jppExAqulrzu805h8iZJF1YPwgCLcBGAsYHQ/s1600/sDAezIApe_pdRnU8tGB0Vfg.png\" width=\"400\">\n",
    "\n",
    "#### Converter\n",
    "\n",
    "To generate a transfer learning model for your task, you need to pick two models that will form it:\n",
    "* __Base model__ that is typically a deep neural network pre-trained on a generic data-rich task.\n",
    "* __Head model__ that takes the features produced by the base model as inputs and learns from them to solve the target personalized task. This is typically a simpler network with a few fully-connected layers.\n",
    "\n",
    "The model should be created with TensorFlow.\n",
    "\n",
    "#### Android library\n",
    "\n",
    "The transfer learning model produced by the transfer learning converter cannot be used directly with the TensorFlow Lite interpreter. An intermediate layer is required to handle the non-linear lifecycle of the model. This is done by the Android library (iOS is not yet supported). The Android library is hosted as a part of the example, but it lives in a stand-alone Gradle module so it can be easily integrated into any Android application.\n",
    "\n",
    "#### Application\n",
    "\n",
    "The Android application shows how to light-retrain a model (transfer learning) and do inference using that model.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Run this app on your device and understand the source code:</b>\n",
    "<ul><li>\n",
    "<a href=\"https://github.com/tensorflow/examples/tree/master/lite/examples/model_personalization\">TensorFlow Lite Example On-device Model Personalization</a>\n",
    "</li></ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Your Task: Activity Monitoring & Transfer Learning (Option 1)\n",
    "\n",
    "<img src=\"img/project_options4.png\" width=\"800\">\n",
    "\n",
    "## Cookbook\n",
    "\n",
    "#### Step 1: Create a base model using an existing data set\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Follow the steps to train a base (=general) model</b>\n",
    "<ul><li>\n",
    "<a href=\"2020-04-20__WS5_1__Transfer_Learning__Base_Model.ipynb\">Transfer Learning - Base Model</a>\n",
    "</li></ul>\n",
    "</div>\n",
    "\n",
    "Feel free to modify the model and improve it.\n",
    "\n",
    "#### Step 2: Create a simple head model\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Follow the steps to create a simple head model</b>\n",
    "<ul><li>\n",
    "<a href=\"2020-04-20__WS5_2__Transfer_Learning__Head_Model.ipynb\">Transfer Learning - Head Model</a>\n",
    "</li></ul>\n",
    "</div>\n",
    "\n",
    "Feel free to modify the model and extend it to 4 classes (it currently supports 2).\n",
    "\n",
    "#### Step 3: Integrate transfer learning into your own app\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Run an example app with your custom-made model with 2 classes</b>\n",
    "<ul><li>\n",
    "<a href=\"2020-04-20__WS5_3__Transfer_Learning__Application.ipynb\">Transfer Learning - Android Application</a>\n",
    "</li></ul>\n",
    "</div>\n",
    "\n",
    "Extend your own activity recognition app (which uses kNN) to provide the following functionality:\n",
    "* Continue using kNN implementation\n",
    "* Add inference using a generic model trained in step 1\n",
    "* Use transfer learning to support data gathering, training and inference for the personalized model\n",
    "* In the GUI show activity prediction using all three models\n",
    "\n",
    "#### Step 4: Compare model performance\n",
    "\n",
    "Compare the performance of the three models on a self-defined set of tests. Log test result into a file and present these in your report!\n",
    "\n",
    "#### Step 5: Final demo\n",
    "\n",
    "Show all models in action in the final demo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* TensorFlow Blog: [Example on-device model personalization with TensorFlow Lite](https://blog.tensorflow.org/2019/12/example-on-device-model-personalization.html), 2019\n",
    "* Medium: [Transfer Learning with keras](https://medium.com/analytics-vidhya/transfer-learning-with-keras-9a1b3253211c), 2019\n",
    "* Medium: [Transfer Learning — part 1](https://medium.com/dataswati-garage/transfer-learning-part-1-c2f87de8df38), 2018\n",
    "* Medium: [Transfer Learning — part 2](https://medium.com/dataswati-garage/transfer-learning-part-2-zero-one-few-shot-learning-8d23d2f8583b), 2018\n",
    "* Medium: [Human Activity Recognition (HAR) Tutorial with Keras and Core ML (Part 1)](https://towardsdatascience.com/human-activity-recognition-har-tutorial-with-keras-and-core-ml-part-1-8c05e365dfa0)\n",
    "* [Continual Learning in Neural Networks](https://arxiv.org/pdf/1910.02718.pdf), KU Leuven, 2019\n",
    "* GitHub: [Human Activity Recognition Tutorial with Keras and CoreML (Part 1)](https://github.com/ni79ls/har-keras-coreml/blob/master/Human%20Activity%20Recognition%20with%20Keras%20and%20CoreML.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
